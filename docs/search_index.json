[["index.html", "Telematics car driving data analytics Preface", " Telematics car driving data analytics Guangyuan Gao 2021-12-09 15:31:40 Preface This work is supported by the SOA research grant. The aggregated compulsory third party liability data contains \\(2119\\) cars with \\(4804\\) years-at-risk and \\(1059\\) claims. The empirical claims frequency is \\(0.22\\). The claims frequency data of cars is saved as car_compulsory.csv. The claims frequency data of commercial policies is saved as policy_commercial.csv. In the claims frequency analysis we may just study the claims frequency of TPL and property and treat the other coverages as risk factors. "],["introduction.html", "Section 1 Introduction", " Section 1 Introduction Guangyuan Gao, Yanlin Shi, He Wang With the advancements of telematics technology, insurers can collect detailed car driving information. Traditionally, insurers use classical actuarial risk factors to classify policyholders and charge different premiums for different risk groups. Telematics car driving data is a very personalized data which encodes driving differences not contained in classical social-demographic risk factors such as regions and ages. For example, some young drivers may have a cautious driving style, while some matured drivers may still have a wild and aggressive driving behavior. In this project, we explore first steps on how such information can be learned from telematics car driving data. Our ultimate goal is to improve auto risk classification models by using telematics car driving data. Although there are concerns with using personalized data and algorithmic prediction (Mahapatra, 2019; Cevolini and Esposito, 2020), we prefer the statement by Cather (2020) that incorporating telematics data into auto insurance risk classification systems would “minimize insurance discrimination and increase cream skimming adverse selection”. Telematics car driving data analysis is far from being trivial because the data comes with all its challenges such as big data (we typically have TBs of data that needs to be processed), data error, etc. According to the current actuarial literature, among others, there are three options to learn from telematics car driving data: (a) Weidner et al. (2016, 2017) extract covariates from time series of telematics data using discrete Fourier transforms; (b) Huang and Meng (2019), Paefgen et al. (2014) and Verbelen et al. (2018) calculate summary statistics of telematics data; (c) Gao et al. (2020) apply convolutional neural networks to learn patterns in speed-acceleration heatmaps. Those papers research on driving styles. Another stream of literature study risk exposure of driving distances or time exposures; see Ayuso et al. (2016a,b), Boucher et al. (2017), and Lemaire et al. (2016). Telematics data is a posterior experience and Denuit et al. (2019) propose a credibility model to incorporate the posterior information of driving behavior. Guillen et al. (2020) study the association of telematics car driving data with near-miss events such as cornering, braking, and accelerating. Geyer et al. (2020) study the effect of driving behavior on risk and insurance selection. Richman (2020) discuss possible approaches to analyze telematics car driving data. Our data is collected by on-board diagnostics (OBD) systems rather than smartphones. There are several works on smartphone-based telematics in the IEEE Transactions journals. Wahlström et al. (2017) summarize researches on smartphone-based vehicle telematics such as vehicle navigation, driver classification, road condition monitoring. Wahlström et al. (2018) study fusion of OBD and GNSS measurements of speed. Wahlström et al. (2015) detect dangerous cornering using GNSS data. The qualities of the IEEE conference papers on telematics car driving data are quite mixed. Among others, Savelonas et al. (2020), Girma et al. (2019) and Carvalho et al. (2020) apply recurrent neural networks to identify drivers or to learn different driving behaviors such as normal, moderate, aggressive, etc. In transportation field, Joubert et al. (2016), Ma et al. (2018), and Hu et al. (2019) study driving behaviors and find certain driving behaviors are more related to at-fault accidents. They use both telematics data and contextual data such as road conditions, traffic flow, speed limits. Ho et al. (2014), Hung et al. (2007), Kamble et al. (2009) study telematics data and driving cycles to understand vehicular emissions, energy consumption and impacts on traffic in different cities of the world. According to the current literature, we list two public telematics car driving data sets: naturalistic driving study dataset and UAH-DriveSet (Romera, 2016). Some of the above literature is based on the two data sets. Unfortunately, our telematics data is not publicly available. References Ayuso, M., Guillen, M., Pérez-Marín, A.M. (2016a). Telematics and gender discrimination: Some usage-based evidence on whether men’s risk of accidents differs from women’s. Risks 4/2, article 10. Ayuso, M., Guillen, M., Pérez-Marín, A.M. (2016b). Using GPS data to analyse the distance traveled to the first accident at fault in pay-as-you-drive insurance. Transportation Research Part C: Emerging Technologies 68, 160-167. Boucher, J.-P., Côté, S., Guillen, M. (2017). Exposure as duration and distance in telematics motor insurance using generalized additive models. Risks 5/4, article 54. Carvalho, E. (2017). Exploiting the use of recurrent neural networks for driver behavior profiling. 2017 International Joint Conference on Neural Networks, 3016-3021. Cather, D.A. (2020). Reconsidering insurance discrimination and adverse selection in an era of data analytics. Geneva Papers on Risk and Insurance - Issues and Practice 45, 426–456. Cevolini, A., Esposito, E. (2020). From pool to profile: Social consequences of algorithmic prediction in insurance. Big Data and Society 7/2. Denuit, M., Guillen, M., Trufin, J. (2019). Multivariate credibility modelling for usage-based motor insurance pricing with behavioural data. Annals of Actuarial Science 13/2, 378-399. Gao, G., Wang, H, Wüthrich, M.V. (2020). Boosting Poisson regression models with telematics car driving data. SSRN ID: 3596034. Geyer, A., Kremslehner, D., Muermann, A. (2020). Asymmetric information in automobile insurance: Evidence from driving behavior. Journal of Risk and Insurance 87/4, 969-995. Girma, A., Yan, X., Homaifar, A. (2019). Driver identification based on vehicle telematics data using LSTM-recurrent neural network. 2019 IEEE 31st International Conference on Tools with Artificial Intelligence, 894-902. Guillen, M., Nielsen, J.P., Pérez-Marín, A.M., Elpidorou, V. (2020). Can automobile insurance telematics predict the risk of near-miss events? North American Actuarial Journal 24/1, 141-152. Ho, S.-H., Wong, Y.-D., Chang, V. W.-C. (2014). Developing Singapore driving cycle for passenger cars to estimate fuel consumption and vehicular emissions. Atmospheric Environment 97, 353-362. Hu, X., Zhu, X., Ma, Y.-L., Chiu, Y.-C., Tang, Q. (2019). Advancing usage-based insurance – a contextual driving risk modelling and analysis approach. IET Intelligent Transport Systems 13/3, 453-460. Huang, Y., Meng, S. (2019). Automobile insurance classification ratemaking based on telematics driving data. Decision Support Systems 127. Hung, W.T., Tong, H.Y., Lee, C.P., Ha, K., Pao, L.Y. (2007). Development of practical driving cycle construction methodology: a case study in Hong Kong. Transportation Research Part D: Transport and Environment 12/2, 115-128. Joubert, J. W., de Beer, D., de Koker, N. (2016). Combining accelerometer data and contextual variables to evaluate the risk of driver behaviour. Transportation Research Part F: Traffic Psychology and Behaviour 41/A, 80-96. Kamble, S.H., Mathew, T.V., Sharma, G.K. (2009). Development of real-world driving cycle: case study of Pune, India. Transportation Research Part D: Transport and Environment 14/2, 132-140. Lemaire, J., Park, S.C., Wang, K. (2016). The use of annual mileage as a rating variable. ASTIN Bulletin 46, 39-69. Ma, Y.-L., Zhu, X., Hu, X., Chiu, Y.-C. (2018). The use of context-sensitive insurance telematics data in auto insurance rate making. Transportation Research Part A: Policy and Practice 113, 243-258. Mahapatra, A. (2019). Privacy: A growing risk in the insurance industry. Creative Components 410. Paefgen, J., Staake, T., Fleisch, E. (2014). Multivariate exposure modeling of accident risk: Insights from pay-as-you-drive insurance data. Transportation Research Part A: Policy and Practice 61, 27-40. Richman, R. (2020). AI in actuarial science - a review of recent advances. Annals of Actuarial Science. Romera, E., Bergasa, L.M., Arroyo, R. (2016). Need data for driver behaviour analysis? Presenting the public UAH-DriveSet. 2016 IEEE 19th International Conference on Intelligent Transportation Systems, 387-392. Savelonas, M. et al. (2020). Hybrid time-series representation for the classification of driving behaviour. 2020 15th International Workshop on Semantic and Social Media Adaptation and Personalization, 1-6. Verbelen, R., Antonio, K., Claeskens, G. (2018). Unraveling the predictive power of telematics data in car insurance pricing. Journal of the Royal Statistical Society: Series C (Applied Statistics) 67, 1275-1304. Wahlström, J., Skog, I., Händel, P. (2017). Smartphone-Based Vehicle Telematics:A Ten-Year Anniversary. IEEE Transactions on Intelligent Transportation Systems 18/10, 2802-2825. Wahlström, J., Skog, I., Händel, P. (2015). Detection of Dangerous Cornering inGNSS-Data-Driven Insurance Telematics. IEEE Transactions on Intelligent Transportation Systems 16/6, 3073-3083. Wahlström, J., Skog, I., Nordström, R.L., Händel, P. (2018). Fusion of OBD and GNSS Measurements of Speed. IEEE Transactions on Instrumentation and Measurement 67/7, 1659-1667. Weidner, W., Transchel, F.W.G., Weidner, R. (2016). Classification of scale-sensitive telematic observables for riskindividual pricing. European Actuarial Journal 6/1, 3-24. Weidner, W., Transchel, F.W.G., Weidner, R. (2017). Telematic driving profile classification in car insurance pricing. Annals of Actuarial Science 11/2, 213-236. "],["overview.html", "Section 2 Overview 2.1 Vehicle identification number data 2.2 Policy data 2.3 Claim data 2.4 Telematics data", " Section 2 Overview From a Chinese domestic insurance company, we have obtained four data sets: (1) the vehicle identification number data (VIN data), (2) the policy data, (3) the claim data and (4) the telematics data. VIN data (1) contains a mapping from the policy code Policy_Code to the vehicle identification numbers Device_ID. We use (1) to aggregate policies of the same car driver and to link policy data (2) with telematics data (3). We use (2) and (3) to create the claims frequency data. For the telematics data (4), more data cleaning and pre-processing are needed. 2.1 Vehicle identification number data In the VIN data, we have 12196 policies on 2311 cars. So averagely one car has bought 5 policies. The cars are identified by the vehicle identification numbers (VINs). This data establishes a mapping from the policies to the vehicle identification numbers. 2.2 Policy data There are two types of policies: the compulsory third party liability (CTPL) policies and the commercial policies. While a CTPL policy has only one CTPL coverage, a commercial policy may have 7 coverages including third party liability (TPL), property, driver, passengers, theft and robbery (TandR), glass and scratch. We show the range of underwriting dates, policy inception dates and expiration dates in Table 2.1. Although not indicated by the insurance company, the observation period should be from 2014-01-01 to 2017-06-30. After removing the policies with zero premium, there are 6032 CTPL policies and 6005 commercial policies on the 2311 cars of the VIN data. Note that 159 policies in the VIN data are missed in the policy data. Also note that those numbers may change during the data cleaning. The collecting information for the two types of policies are different. In the next two chapters, we will discuss the two types of policies separately. 2.3 Claim data There are 1031 CTPL policies and 1690 commercial policies having made at least one claim. In the claim data, we have 1204 CTPL claims, and 2218 commercial claims. Note that most policies do not incur a claim and a policy can incur more than one claims. A CTPL claim contains only the CTPL coverage, while a commercial claim may contain up to 7 coverages. We provide the range of accident dates, reporting dates and claim closure dates in Table 2.1. The exposure period should be from 2014-01-01 to 2017-06-30. Note that 28 claims are still open on 2017-06-30. Table 2.1: The range of underwriting dates (UNDERWRITEENDATE), policy inception dates (KINDSTARTDATE), policy expiration dates (DUEENDDATE), accident dates (DAMAGEDATE), reporting dates (REPORTDATE) and the claim closure dates (ENDCASEDATE) dates min max UNDERWRITEENDDATE 2013-10-19 2017-06-30 KINDSTARTDATE 2014-01-01 2017-09-25 DUEENDDATE 2014-03-22 2018-09-24 DAMAGEDATE 2014-01-09 2017-06-29 REPORTDATE 2014-01-09 2017-06-30 ENDCASEDATE 2014-01-20 2017-06-30 Table 2.2 shows the number of claims for each coverage. We also calculate the proportions of zero claims for each coverage. Note that a large proportion of claims incurring the TPL coverage have zero TPL claims. Note that a claim incurring zero TPL claim does not necessarily incur zero claims for other coverages. Table 2.2: The proportions of zero incurred claims coverages claims counts zero claims proportions of zero claims CTPL 1204 57 0.05 TPL 1041 775 0.74 property 1774 236 0.13 driver 2 1 0.50 passengers 1 0 0.00 TandR 1 0 0.00 glass 34 0 0.00 scratch 21 0 0.00 We calculate the reporting delay (report date - damage date) and payment delay (endcase date - report date) for each coverage in Table 2.3. Note that 99% claims are reported within one week. Table 2.3: The 99% quantiles of reporting delay and payment delay. coverages Delay_Report Delay_Payment CTPL 4 194 TPL 4 209 property 3 145 driver 0 34 passengers 0 2 TandR 0 64 glass 3 9 scratch 2 1 A question of interest is whether policyholders tend to file a claim at the end of policy duration. We draw the histogram of the days between the reporting and the expiration for three main coverages in Figure 2.1. Hence, policyholders do not tend to file a claim at the end of policy duration. Figure 2.1: The histograms of the days between the reporting and the expiration 2.4 Telematics data The telematics data is zipped by days from 2015/04/26 to 2017/07/08, i.e., a zipped folder contains all the trips of all the cars on that day. The daily telematics volume after 2015/08/13 is around 40 folds than that before 2015/08/13. The reason is that there is device installing rush in 2015/07/08. We save the telematics data before 2015/08/13 to the address ./telematics_data/Incomplete_before_2015_08_13/SUNSJRN_Date.00.zip. We also have the summary data and event data for all the trips grouped by days. The summary data is used to validate the detailed telematics data and the event data records the pre-defined abnormal car moving events. We do not study these two data sets furthermore. To be consistent with the claims frequency data where each row is for a policy, we group the telematics data by policies. We focus on the telematics data for the compulsory policies. We create a car folder Device_ID for each car and in this car folder we create a policy folder Policy_Code for each policy of this car. We extract all the trips of this car during this policy effective period, and save them into the corresponding policy folder (i.e., in the address of ./telematics_data/compulsory_car/Device_ID/Policy_Code/Device_ID_Trip_Number.csv). Note that a trip file is named as Device_ID_Trip_Number.csv. We will explain the Trip_Number later. We save the trips not in any policies in a folder of other_trips under the car folder (i.e., in the address of ./telematics_data/compulsory_car/Device_ID/other_trips/Device_ID_Trip_Number.csv). We have 2296 cars with compulsory policies, in which 2131 cars have telematics data and 165 cars have no telematics data (i.e., 7% car folders ./telematics_data/compulsory_car/Device_ID/ contain zero telematics files). There are 1527 policies having no telematics data, amounting for 26% of the total 5904 policies. The reason of this high missing percentage is that the policies’ effective dates are ranging from 2014-01-01 to 2017-06-25 but the telematics data is available for a shorter period from 2015/08/13 to 2017/07/08. Therefore it is better to work on the aggregated policy data on cars. During the aggregation, we need to consider the change of driver which actually rarely happen. By working on the aggregated policies on cars, we effectively interpolate the telematics covariates for policies before 2015-08-13 and only 7% cars miss the telematics covariates. Not all the car driving trips during the policy effective period are recorded, since the telematics data collecting devices may be installed anytime during the policy effective period. Therefore it is difficult to calculate the total driving distance during a policy period. We will use average driving distance per day or average driving time per day instead. In the following, we draw the distribution of trips: (top-left) the total trips per car with CTPL policies; (top-right) the trips per CTPL policy; (bottom-left) the trips per car with CTPL policies during no CTPL policies period; (bottom-right) the trips per car without CTPL policies. Figure 2.2: The distribution of number of trips The telematics data of the 1541 cars without compulsory policies is saved in the folder ./telematics_data/non_CTP_trips with the sub-folders /Device_ID/Device_ID_Trip_Number.csv denoting the cars. "],["compulsory-third-party-liability-policies.html", "Section 3 Compulsory third party liability policies 3.1 Data cleaning 3.2 Policies aggregation w.r.t cars", " Section 3 Compulsory third party liability policies In this section we construct the claims frequency data for the compulsory third party liability policies. We discuss the data cleaning and perform some preliminary summary analysis. We establish the claims frequency of the CTPL policies using the policy data (6032 policies on 2298 cars) and the claim data (1204 claims). All the CTPL policies have one coverage of third party liability and have the same amount of insurance CNY 122000. In 2021 all the Chinese insurance companies increase the CTPL AOI to CNY 200000. 3.1 Data cleaning There are 26 variables for each CTPL policy. We describe those variables and the pre-process procedure as follows. 3.1.1 Identification variables Variables 1-2 are the identification variables. Device_ID is used to match the frequency data with the telematics data. Policy_Code identifies each policy. One car may have bought several policies, i.e., renewal policies for several years. 3.1.2 Time Variables 3-5 are the time variables. KINDSTARTDATE is the policy effective date ranging from 2014-01-01 to 2017-09-25. DUEENDDATE is the policy expiring date ranging from 2014-09-02 to 2018-09-24. UNDERWRITEENDATE is the underwriting date ranging from 2013-10-19 to 2017-06-30. 3.1.3 Policy duration Variable 6 is the policy duration. YEARS is the policy duration from KINDSTARTDATE to DUEENDDATE. There are 6 policies with less than 1 year policy duration (no claims made on these policies). We remove those policies, resulting all the policies with 1 year policy duration. 3.1.4 Car features Variables 7-11 are the car variables. PURCHASEPRICENOTAX is the car purchase price, ranging from CNY25000 to CNY990000. SEATCOUNT ranges from 2 to 8, and 96% policies have 5 seats. CARBRAND contains 67 car brands. CARSERIESNAME contains 299 car series. USEYEARS ranges from 0 to 14 years. 3.1.5 Driver features Variables 12-14 are the driver variables. BRANCHNAME is the branch name selling the policy. It indicates the main area where the car is driven. There are 27 different branches. AGE is the driver’ age. After correcting for 6 driver’s ages, the driver’s age ranges from 19 to 77 years old. SEX is the driver’s gender. The number of policies bought by males is nearly as double as the females. 3.1.6 Experience rating factors Variables 15-16 are the experience rating factors. LASTCLAIMCOUNT is the number of claims in the previous year. It ranges from 0 to 4, with -3 indicating a new car and -4 indicating a new policy. We keep the 8 missing values since we will use the no-claim discount (NCD) factor instead. NCD_Compulsory is the no-claim discount (NCD) factor for the compulsory policies. It takes values from {0.7, 0.8, 0.9, 1, 1.1, 1.3}. The relationship between LASTCLAIMCOUNT and NCD_Compulsory is shown in Figure 3.1. Figure 3.1: Last claim count v.s. NCD 3.1.7 Other policy features Variable 17 is the policy type. Variable 18 is the coverage type. Variable 19 is the amount of insurance (AOI). Variable 20 is the indication of non-deductible. Variable 21-22 are the premium. RISKCODE indicates the type of policy, either the CTPL policy or the commercial policy. KINDCODE indicates the coverages of the policy. For the CTPL policy, there is only one coverage, third party liability. AMOUNTNEW is the amount of insurance (the coverage limit). We correct for two data errors. All the CTPL policies have the same coverage limit of CNY122000. FLAG is the indication of non-deductible. We correct for one data error. All the CTPL policies are non-deductible. PREMIUM is the premium for each coverage, ranging from CNY285 to CNY1850. PREMIUM_Total is the total premium for all coverages. For the CTPL policies, it equals to PREMIUM. Figure 3.2 shows that generally the premiums increase with NCD factor. Figure 3.2: NCD v.s. Premiums 3.1.8 Claims features Variable 23-26 are the claim variables extracted from the claim data. Claim_Code contains all the claim codes on this policy. Note that one policy may make several claims. Claim_Coverage contains the incurred coverages of all the claims. For the CTPL policy, it is always the third party liability coverage. Claim_Amount contains the incurred claims amount of each coverage of each claim. Claim_Count is the number of claims made on this policy. This variable is the response variable in the claims frequency modelling. We show the distribution of last reporting dates (from policy inception) in Figure 3.3, which indicates that the last reporting dates are distributed uniformly along the policy duration. After the data cleaning, we have 6026 policies on 2296 cars in which 1204 claims are made by 1031 policies on 829 cars. Figure 3.3: The distribution of last reporting dates Considering the report delay as shown in Table 2.1, we set the observed exposed period as from 2014-01-01 to 2017-06-25. There are 122 policies outside the observed exposed period and 1614 policies are partially exposed. The total claims count is 1204 and the total exposure is 5262. The claims frequency is 0.23 on those 5904 policies of 2296 cars. The claims frequency data of CTPL policies is saved as policy_compulsory.csv 3.2 Policies aggregation w.r.t cars As we discussed before, there is a large proportion of policies (\\(26\\%\\)) missing the telematics data. We aggregate policies with respect to the cars Device_ID. We need to check the cars with multiple drivers and implement appropriate data cleaning. We can detect the driver changes using the region, the age, and the gender. There are 8 cars changing the region, 82 cars with drivers changing the gender and 87 cars with drivers changing the age for more times than the number of policies. We remove these cars. We set the age of drivers with multiple policies as the median of ages. The car price PURCHASEPRICENOTAX is the market value which changes yearly, and we set it as the median. We set the use-years USEYEARS, the no-claim discount factor NCD_Compulsory as the mean. We calculate the total premium PREMIUM_Total, the total exposure Earned_Years, and the total claim counts Claim_Count for each car. Finally we have removed 177 cars. The aggregation data contains 2119 cars with 4804 years-at-risk and 1059 claims. The empirical claims frequency is 0.22. The claims frequency data of cars is saved as car_compulsory.csv. Remarks: It may be that driver’s information has changed for optimizing the insurance premium from a policyholder perspective. In a second stage one may analyze whether this change of policyholder is caused by an accident, i.e. whether there was an accident immediately before the change. In a third stage we may test whether the driving behavior changes, i.e. is this really a change of car driver or only a change on paper (policy contract). "],["commercial-policies.html", "Section 4 Commercial policies 4.1 Data cleaning 4.2 Comparison of the 7 coverages", " Section 4 Commercial policies In this section we construct the claims frequency data for the commercial policies. We discuss the data cleaning and perform preliminary summary analysis. From Table 2.2, we have seen that the claims frequency for driver, passengers, theft and robbery, glass, and scratch is quite small. It is impossible to set up claims frequency models for those coverages. We will focus on the claims frequency model of TPL and property coverages. The other coverages will be treated as policy attributes. We remove the 61 coverages with non-positive premiums. There are 20498 coverages of 6005 commercial policies on 2294 cars. We aggregate the coverages with respect to policies, i.e., each row of the original data is one coverage of a policy and each row of the new data is a policy. In a commercial policy, people can choose any coverages from the 7 coverages: TPL, property, driver, passengers, theft and robbery, glass, and scratch. 4.1 Data cleaning The commerical policy data has the similar variables to the CTPL data such as Device_ID, Policy_Code, KINDSTARTDATE, DUEENDATE, UNDERWRITTENDATE, YEARS, PURCHASEPRICENOTAX, SEATCOUNT, CARBRAND, CARSERIESNAME, USEYEARS, BRANCHNAME, AGE, SEX, LASTCLAIMCOUNT, NCD_Compulsory (changed to NCD_Commercial), RISKCODE, PREMIUM_Total. The variables KINDCODE, AMOUNTNEW, FLAG, PREMIUM, Claim_Count need to split into 7 components corresponding to the 7 coverages. We discuss the data cleaning as following. We correct for 3 age errors, 2 missing ages, 2 missing gender and 1 AOI error of passenger coverage. The no-claim discount factor for commercial policies takes value from {0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 1, 1.1, 1.2, 1.25, 1.3, 1.5}. The relationship between NCD_Commerical and LASTCLAIMCOUNT is shown in Figure 4.1. Figure 4.1: Last claim count v.s. NCD. We investigate the policies with zero AOI for coverages property, driver, passenger, scratch and find that all of them (except for one) have less than one year exposure. Therefore we remove the 114 policies with less than one year exposure. There is a policy missing the AOIs of property damage and theft and robbery. This policy made the only one theft and robbery claim of the whole portfolio, and the claim amount is even bigger than the car purchase price. We remove this policy. We remove 6 policies with more than one year exposure, and remove 2 policies without the TPL coverages. We now finish the data cleaning for the commercial policies and all the commercial policies have one year exposure. We set the observed exposed period as from 2014-01-01 to 2017-06-25. So there are 134 policies outside the observed exposed period and 1552 policies are partially exposed. The claims frequency data of commercial policies is saved as policy_commercial.csv. 4.2 Comparison of the 7 coverages The distribution of coverages across the policies is shown in Figure 4.2. All policies contain the TPL coverage and 846 (0.15) policies do not have property coverages. Very few policies contain theft and robbery, glass damage and scratch coverages. Figure 4.2: The distribution of coverages across the policies. We compare the 7 coverages in terms of policy numbers, exposure, claim frequency, and non-deductible in Table 4.1. The claims frequency of each coverage is calculated on the policies containing the corresponding coverage. The claims frequency for scratch is the highest followed by the property damage. Almost all the policies choose non-deductible for TPL, property, driver and passengers. Around 40% policies choose deductible for theft and robbery and scratch. There is no deductible information for glass damage. Table 4.1: Comparison of 7 coverages. coverages pol_no. pol_% exposures claims frequency Non_ded. TPL 5748 100.00 5131 1033 20.13 99.53 Property 4902 85.28 4410 1752 39.73 99.49 Driver 3707 64.49 3354 2 0.06 99.33 Passengers 3526 61.34 3191 1 0.03 99.49 TandR 916 15.94 845 0 0.00 37.12 Glass 891 15.50 824 34 4.13 0.00 Scratch 52 0.90 49 21 42.61 42.31 The amount of insurance (AOI) for TPL, driver, passengers and scratch is fixed to certain levels: TPL {50000, 100000, 150000, 200000, 300000, 400000, 500000, 1000000, 1500000, 2000000} driver {10000, 20000, 30000, 50000, 60000, 100000} passenger {10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 120000, 140000, 180000, 200000, 300000, 400000} scratch {2000, 5000}. The AOIs of property and theft and robbery have a linear positive relationship with the purchase price as shown in Figure 4.3 Figure 4.3: Pair plots of car price, aoi of property, and aoi of theft and robbery. We show the logarithm of premiums for each coverage in Figure 4.4. The TPL and property are the most popular coverages. Though they are expensive most people buy the two coverages. The driver and passengers are not very useful (very low frequency) but cheap. So a few people also buy them. The theft and robbery, glass and scratch are more expensive than driver and passengers, so most people do not buy them. In the claims frequency analysis we may just study the claims frequency of TPL and property and treat the other coverages as risk factors. Figure 4.4: The distribution of the logrithm of premium for each coverage. "],["telematics-car-driving-data.html", "Section 5 Telematics car driving data 5.1 Telematics variables 5.2 Telematics data cleaning 5.3 Three formats of telematics data", " Section 5 Telematics car driving data Guangyuan Gao, Yanlin Shi, He Wang In this chapter, we first describe the telematics variables collected. Then we conduct the data cleaning. Finally we get three formats of telematics data: (1) time series; (2) summary statistics; (3) heatmaps. 5.1 Telematics variables A trip is defined as the period from the engine start to the engine switch off. During a trip, the following 15 variables are recorded second by second. We explain them as follows. 5.1.1 Field mask Field_Mask. This is a hex code which can be converted into a binary vector indicating the validation of the telematics variable 6. GPS_Latitude to 15. Accel_Vertical (using function BMS::hex2bin). Note that the original data records the invalid data as zero, and we need to change those invalid data as NA. 5.1.2 Vehicle identification variables Device_ID. This variable also appears in the userlist dataset. It is used to match the telematics data with the policy data. Detected_VIN. Given the Device_ID, this variable is not very useful in the current study. 5.1.3 Time Trip_Number. This variable is the beginning time of a trip in the UTC format. This variable has the same value during the whole trip. It can be converted to the Beijing time by as.POSIXlt(,origin=\"1970-01-01\",tz=\"Asia/Shanghai\"). Time_Stamp. This variable is the UTC time of the corresponding record in a trip. This variable is increasing by one in a trip file. 5.1.4 GPS variables When the following GPS variables are invalid, they are recorded as zero. To avoid confusion, we change the invalid zero values to NA. GPS_Latitude. This variable is the GPS latitude in decimal degree multiplied by \\(10^7\\). GPS_Longitude. This variable is the GPS longitude in decimal degree multiplied by \\(10^6\\). The GPS spherical coordinates can be converted to the geodetic coordinates \\((x,y)\\) using SoDA::geoXY. The geodetic coordinates can be used to calculate the distance, speed \\(v^{(xy)}\\), angle \\(\\psi^{(xy)}\\), acceleration \\(a^{(xy)}\\) and angle change \\(\\Delta^{(xy)}\\). There are GPS signal drift problems with the GPS coordinates \\((x,y)\\), which will affect those derived variables. GPS_Heading. This variable is the approaching direction of the vehicle \\(\\psi^{(gps)}\\) in decimal degree multiplied by \\(10^2\\). It ranges from 0 to 360. GPS_Speed. This variable is the vehicle speed \\(v^{(gps)}\\) in \\(km/h\\) multiplied by \\(10\\). Note that \\(3.6 km/h = 1 m/s\\). Positional_Quality. This variable indicates the GPS signal quality. When it is zero, the vehicle cannot be located by the GPS satellite and the above four variables are invalid. Sometimes when it is one, the GPS coordinates might be zero. So it is better to judge the GPS validation using both the GPS coordinates and this variable. 5.1.5 Instrument panel variables VSS_Speed. This is the vehicle speed \\(v^{(vss)}\\) shown in the instrument panel. The insurer told us that when this variable is valid it is more reliable than the GPS_Speed \\(v^{(gps)}\\). Engine_RPM. This is the rotation of engine per minute shown in the instrument panel. We do not use this variable in the study. 5.1.6 Accelerometer variables The following three variables are measured by a three-axis accelerometer. Accel_Lateral. The lateral acceleration \\(a&#39;^{(acc)}\\) is perpendicular to the car approaching direction, which measures the change of car approaching direction. Accel_Longitudinal. The longitudinal acceleration \\(a^{(acc)}\\) is along the car approaching direction, which measures the change of speed values. Accel_Vertical. The vertical acceleration is always around the gravity \\(9.8\\)m/s\\(^2\\). There is calibration bias with these variables as we will see later. And it is difficult to deal with this issue. Therefore, we will use the derived longitudinal acceleration rates \\(a^{(gps)}, a^{(vss)}, a^{(xy)}\\) which are obtained from the GPS speed \\(v^{(gps)}\\), the VSS speed \\(v^{(vss)}\\), and the GPS coordinates \\((x,y)\\), respectively. And we will replace the lateral acceleration by the angle changes of heading \\(\\Delta^{(gps)},\\Delta^{(xy)}\\), which are obtained from the GPS heading \\(\\psi^{(gps)}\\) and the GPS coordinates \\((x,y)\\), respectively. 5.2 Telematics data cleaning Telematics car driving data is a big data. Data cleaning for big data is very challenging since we need to apply the same cleaning procedure to all the trips of all cars. The data cleaning should be flexible enough in order to be applied to all trips. On the other hand, according to the large number theorem, if there were no systematic data error, we could get reliable results using summary statistics of telematics variables even though we did not perform a perfect data cleaning. We first visualize several trips to find what data issues need to be addressed during the data cleaning. Then we design a “naive” data cleaning procedure. We modify the data cleaning procedure by monitoring its performance on selected trips. Finally we get a “universally” applied data cleaning procedure. 5.2.1 Original telematics data We first look at three trips of three drivers in the following Figures. For each trip, we draw the following 6 plots: Top-left: The time series of GPS signal quality, instrument panel signal quality, and accelerometer signal quality; Topright: The trajectory \\((x,y)\\); Middle-left: The time series of GPS speed \\(v^{(gps)}\\), VSS speed \\(v^{(vss)}\\), Middle-right: The time series of GPS heading \\(\\psi^{(gps)}\\); Bottom-left: The time series of longitudinal acceleration \\(a^{(acc)}\\); Bottom-right: The time series of lateral acceleration \\(a&#39;^{(acc)}\\). Figure 5.1: One trip of driver 8 Figure 5.1 shows a very short trip of 2 minutes for driver 8. The top-left plot shows that the vss signal is missing in the last 40 seconds. The top-right and middle-right plots show that the vehicle starts from east-south and approaches to west-north. Note that there are jumps between \\(\\psi^{(gps)}=0\\) and \\(\\psi^{(gps)}=360\\). The middle-left plot shows that the GPS speeds match with the VSS speeds and the VSS speeds are missed for the last 40 seconds. The bottom two plots show that there are calibration bias with both the acceleration rates. We might correct for the bias by subtracting the median of acceleration rates. Figure 5.2: Two trips of driver 8 Figure 5.2 shows another two trips of driver 8. For the left trip, the bottom two plots indicate the calibration bias of accelerometer again. For the right trip, there are several parts missing the GPS signal. We need to interpolate the GPS coordinates, the GPS speed and heading when the GPS signal is missing. Figure 5.3: Three trips of driver 288 Figure 5.3 shows three trips of driver 288. The GPS signals seem stable for the three trips, and the accelerometer seems work well except for the lateral acceleration of the second trip. Figure 5.4: Three trips of driver 1188 Figure 5.4 shows three trips of driver 1188. The GPS signals are unstable in the first two trips, and the VSS signals are unstable for the last two trips. There seems calibration bias of the acceleration rates for all these trips. In a summary, we need to do the following data cleaning: There are missing values in GPS coordinates \\((x,y)\\), GPS speed \\(v^{(gps)}\\), GPS heading \\(\\psi^{(gps)}\\), and VSS speed \\(v^{(vss)}\\). We need to interpolate these missing values. There are frequent calibration problems with the accelerometer variables \\(a^{(acc)},a&#39;^{(acc)}\\). It is difficult to remove the calibration bias since the incurring time and the severity of calibration bias are rather random. We’d better use other variables to describe the acceleration in the two directions. 5.2.2 Data imputation Data imputation is necessary if we analyze the time series of telematics variables. However, if we are only interested in summary statistics of telematics variables such as mean of speed, standard deviation of acceleration, we do not need to interpolate the missing values according to the law of large number. We linearly interpolate the missing values of GPS coordinates \\((x,y)\\), GPS speed \\(v^{(gps)}\\), GPS heading \\(\\psi^{(gps)}\\), and VSS speed \\(v^{(vss)}\\), respectively. Note that when the missing values incur at the boundaries of a trip, we cannot interpolate them. Remarks: The imputation of GPS heading around 0 or 360 degree is difficult. However, in our study we are more interested in the angle change and we take sin of the angle change. Those calculations reduce the effects of wrong GPS heading imputation. GPS drifts may affect GPS speed, angle change and acceleration. However, GPS drifts does not happen frequently. 5.2.3 Derived acceleration and angle change Since the quality of accelerometer variables are poor, we need to find other variables to measure the acceleration and direction changes. We calculate the acceleration rate by using GPS speed \\(v^{(gps)}\\), VSS speed \\(v^{(vss)}\\) or GPS coordinates \\((x,y)\\): \\[\\begin{equation} \\begin{aligned} a^{(gps)}_t&amp;=v^{(gps)}_{t}-v^{(gps)}_{t-1} \\\\ a^{(vss)}_t&amp;=v^{(vss)}_{t}-v^{(vss)}_{t-1} \\\\ a^{(xy)}_t&amp;=v^{(xy)}_{t}-v^{(xy)}_{t-1} \\end{aligned} \\end{equation}\\] where \\[v^{(xy)}_t=\\sqrt{(x_t-x_{t-1})^2+(y_t-y_{t-1})^2}.\\] We measure the direction change by the angle change which can be calculated using either GPS heading \\(\\psi^{(gps)}\\) or GPS coordinates \\((x,y)\\): \\[\\begin{equation} \\begin{aligned} \\Delta^{(gps)}_t&amp;=\\arcsin\\left( \\sin\\left(\\psi^{(gps)}_{t}-\\psi^{(gps)}_{t-1}\\right)\\right) \\\\ \\Delta^{(xy)}_t&amp;=\\arcsin\\left(\\sin\\left(\\psi^{(xy)}_{t}-\\psi^{(xy)}_{t-1}\\right)\\right) \\end{aligned} \\end{equation}\\] where \\[\\psi^{(xy)}_t=\\mathbb{1}_{(-\\infty,0)}\\left(\\arctan2\\left(\\frac{x_t-x_{t-1}}{y_t-y_{t-1}}\\right)\\right)\\times2\\pi+\\arctan2\\left(\\frac{x_t-x_{t-1}}{y_t-y_{t-1}}\\right)\\in[0,2\\pi].\\] Note that a positive value of \\(\\Delta\\) indicates a right turn while the negative one indicates a left turn. The jump between 0 and 360 is not an issue since we take sin. Remarks: Be careful when converting speed units from km/h to m/s, and angle units from decimal degree to radian. The original GPS coordinates is spherical coordinates and they need to be converted to the geodetic coordinates before doing the above calculation. 5.2.4 Selection of telematics variables We discuss the selection among the telematics variables: speed \\(v^{(gps)},v^{(vss)},v^{(xy)}\\), acceleration \\(a^{(gps)},a^{(vss)},a^{(xy)}\\), angle \\(\\psi^{(gps)},\\psi^{(xy)}\\), and angle change \\(\\Delta^{(gps)},\\Delta^{(xy)}\\). After data imputation and deriving the corresponding telematics variables, we draw the same trips as those in Figures 5.1,5.2, 5.3 and 5.4. Figure 5.5: One trip of driver 8 The top two plots in Figure 5.5 are exactly the same as the top two in Figure 5.1. In the middle-left plot, we add \\(v^{(xy)}\\) for comparison. Note that there is a GPS drift around 60 second causing a jump of \\(v^{(xy)}\\). In the middle-right plot, we add \\(\\psi^{(xy)}\\) for comparison. The derived heading direction \\(\\psi^{(xy)}\\) is always zero when the vehicle stands still after 50 seconds. In the bottom plots we show the derived acceleration \\(a^{(gps)},a^{(vss)},a^{(xy)}\\) and the derived angle change \\(\\Delta^{(gps)},\\Delta^{(xy)}\\). Figure 5.6: GPS drift We investigate the GPS drift around 60 seconds. In Figure 5.6, we draw the time series of \\((x,y)\\) coordinates, \\(v^{(xy)}\\) and \\(\\psi^{(xy)}\\) from 45 second to 65 second, respectively. We see a jump of \\((x,y)\\) at 62 second which leads to an extremely speed jump from 0 to more than 200 km/h and an extremely direction jump from 0 to more than 60 degree in a second. We conclude that the derived variables from GPS coordinates are unstable compared with the derived variables from GPS speed, GPS heading and VSS speed. This is due to the measurement error of GPS coordinates and its leverage effects on the acceleration and angle change. Figure 5.7: Two trips of driver 8 For the second trip of driver 8, we see a GPS drift around 70 second. For the third trip of driver 8, the imputation works very well for GPS coordinates, GPS speed and GPS heading. GPS drifts are often observed when there is a speed peak. The heading \\(\\psi^{(xy)}\\) derived from GPS coordinates is always incorrectly zero when the vehicle stops. Again, the acceleration and angle change derived from GPS coordinates are very unstable. Figure 5.8: Three trips of driver 288 Figure 5.9: Three trips of driver 1188 There are no new observations in Figures 5.8 and 5.9. In a summary, we conclude that The linearly imputation works well. Due to the GPS coordinates drift, we should avoid to use the variables derived from \\((x,y)\\). Instead, the telematics variable \\(v^{(vss)}, v^{(gps)}, a^{(vss)}, a^{(gps)}, \\psi^{(gps)}, \\Delta^{(gps)}\\) are reliable. The distance should be derived using the speed variable rather than the coordinates. The three variables \\(v,a,\\Delta\\) are related to driving behavior, while the direction \\(\\psi^{(gps)}\\) is not relevant. We should use the variables from the same sensor in a certain study, i.e., either \\((v^{(vss)},a^{(vss)})\\) or \\((v^{(gps)},a^{(gps)},\\Delta^{(gps)})\\). 5.2.5 Saved telematics variables To save the disk space, for each trip we extract the following telematics variables. GPS speed \\(v^{(gps)}\\) GPS heading \\(\\psi^{(gps)}\\) Validation of GPS VSS speed \\(v^{(vss)}\\) Validation of VSS Note that we can derive the acceleration and angle change easily from the speed and heading. Note that if the GPS is invalid, the corresponding variables are recorded as NA. So does the VSS. We name the trip file as Device_ID_Trip_Number.csv, so the Device_ID and the trip beginning time Trip_Number are saved in the file name. We save the preprocessed “thin” trips into ./telematics_data/compulsory_cleaned/Device_ID/Policy_Code/Device_ID_Trip_Number.csv. We draw the speed \\(v^{(gps)}\\), heading direction \\(\\psi^{(gps)}\\), acceleration \\(a^{(gps)}\\), angle change \\(\\Delta^{(gps)}\\) for the previous trips. We denote the times for missing values by a horizontal line at the top of each plot. Note that we have capped the \\(a^{(gps)}\\) between \\((-4,4)\\)m/s\\(^2\\), and \\(\\Delta^{(gps)}\\) between \\((-45^{\\circ},45^{\\circ})\\). Figure 5.10: Three trips of driver 8 Figure 5.11: Three trips of driver 288 Figure 5.12: Three trips of driver 1188 Finally, we interpolate the missing \\(v^{(gps)}\\) and \\(\\psi^{(gps)}\\) linearly. We calculate \\(a^{(gps)}\\) and \\(\\Delta^{(gps)}\\) from the imputed values. The corresponding figures are shown as follows. Figure 5.13: Three trips of driver 8 Figure 5.14: Three trips of driver 288 Figure 5.15: Three trips of driver 1188 5.3 Three formats of telematics data We consider three formats of telematics data. The first format is time series. The second format is summary statistics. The third format is heatmap. The data imputation is important for the first format, while the imputation is not so important for the second and the third format. Hence, to get the second and third formats, several steps above can be skipped, i.e., we select the valid GPS variables and discard the invalid GPS variables and we do not need to perform the data imputation. No matter which format is used, we must consider the speed interval and cap the acceleration and angle change. A particular speed interval makes the comparison among drivers more sensible, and truncation eliminates the leverage effects of outliers. The activation functions in the neural networks play the similar role. 5.3.1 Time series of individual trips An observation from Figure 5.1 is that a trip may contain several standing still phases \\(\\{t: v_t=0,a_t=0\\}\\). Those phases should be removed since they have nothing about the driving behavior. Also note that when the car speed is slow, the GPS tends to be more unreliable and the angle change tends to be large, e.g., parking. Suppose we consider the driving behavior in the speed interval \\([10,60]\\) km/h. By focusing on a particular speed bucket, we can make more sensible comparison among drivers. We first extract partial trips in \\([8, 62]\\) km/h, and then set the acceleration and angle change in the shifting parts \\([8,10] \\cup [60,62]\\) km/h as zero. Finally, we set the speed as \\(10\\) and \\(60\\) km/h in the shifting parts \\([8,10] \\cup [60,62]\\) km/h, respectively. We save the first \\(\\tau=5\\times60\\) seconds of such concatenated trips for each trip. Note that for the trips considered above, we obtain the following three concatenated trips in Figure 5.16. The other trips do not spend at least 5 minutes in speed interval \\([10,60]\\) km/h. Figure 5.16: The third trip of driver 288. The first and second trips of driver 1188. We extract at most 500 trips for each car in the folder ./telematics_data/compulsory_concatenated/Device_ID/Device_ID_Trip_Nubmer.csv. Therefore, for each driver \\(i=1,\\ldots,n\\), we have \\(c_i\\in[0,500]\\) pre-processed trips. The distribution of \\(c_i\\in(0,500)\\) is shown in Figure 5.17. Note that there are 175 cars (8%) without pre-processed trips and there are 1721 cars (75%) cars with 500 pre-processed trips. For driver \\(i\\) with \\(c_i&gt;0\\) trips, we stack its trips into a \\(c_i\\times300\\times3\\) array \\(\\mathbb{z}_i=(z_{i,1},\\ldots,z_{i,c_i})&#39;\\), where each trip \\(z_{i,j}\\in[10,60]^{300}\\times[-4,4]^{300}\\times[-45^{\\circ},45^{\\circ}]^{300}\\). Note that we will use the un-bolded notation \\(z_j\\) to refer to a trip \\(j\\) without considering which driver it belongs to. Figure 5.17: The distribution of pre-processed trips in (0, 500) 5.3.2 Summary statistics of telematics variables We extract several summary statistics for each trip. We categorize those summary statistics into two types: exposure and driving style. We define two status of driving: idle phrase and moving phrase. In the idle phrase, the speed, acceleration and angle change are simultaneously equal to zero. In the moving phrase, at least one of the speed, acceleration and angle change is not equal to zero. Note that we receive for \\(2296-165=2131\\) cars since \\(165\\) cars do not have telematics information. We save them in the folder ./telematics_data/compulsory_statistics/Device_ID.csv. 5.3.2.1 Exposures Total distance in km (1 variable). Total moving time in second (1 variable). Total standing time in second (1 variable). The day of the trip from Monday to Sunday (1 variable). Driving distance split into morning peak (7-9), day (9-17), afternoon peak (17-19), evening (19-22), and night (22-7) (5 variables). Driving time (moving time) split into morning peak (7-9), day (9-17), afternoon peak (17-19), evening (19-22), and night (22-7) (5 variables). In Figure 5.18 we show the distribution of observation period, days having trips, driving hours per day and average daily driving distance. Figure 5.18: The distribution of telematics exposure 5.3.2.2 Driving styles Average/median/standard deviation/extreme values (5% and 95% quantiles) of speed/acceleration/angle change during the driving phrase (15 variables). Driving time (moving time) split into in the speed interval 0-10/10-20/20-30/…/110-120/&gt;120 (13 variables). Driving time (moving time) split into the acceleration intervals (-4)-(-3.5)/(-3.5)-(-2.5)/…/2.5-3.5/3.5-4 (9 variables). Driving time (moving time) split into the angle change intervals (-45)-(-35)/(-35)-(-25)/…/25-35/35-45 (9 variables). 5.3.3 Heatmaps Heatmaps reflect the interaction among speed, acceleration, and angle change. For each car, we construct three heatmaps: speed-acceleration heatmap, speed-angle heatmap, and acceleration-angle heatmap. We use the telematics data in the moving phrase to construct the heatmap. The heatmaps are saved in ./telematics_data/compulsory_speed_accel_angle/Device_ID_accel_angle.csv (Device_ID_speed_accel.csv, Device_ID_speed_angle.csv). The speed interval is divided into sub-intervals at 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 200. The acceleration is divided into sub-intervals at -4, -3.5, -2.5, -1.5, -0.5, 0.5, 1.5, 2.5, 3.5, 4. The angle change is divided into sub-intervals at -45, -35, -25, -15, -5, 5, 15, 25, 35, 45. We typically truncate the speed interval and cap the acceleration and angle change interval. We need to make sure the considered intervals contain sufficient observations to receive stable heatmaps. Usually, we need to normalize the heatmap. We could do the normalization in each speed sub-interval or in several consecutive speed sub-intervals. "],["claims-frequency-modeling.html", "Section 6 Claims frequency modeling 6.1 Data preprocess 6.2 Generalized linear model for claim counts 6.3 Improved GLM with boosting methods 6.4 Improved GLM with telematics individual trips 6.5 Discussions 6.6 Sensitivity analysis and conclusions", " Section 6 Claims frequency modeling We consider the claims history of 1847 cars whose exposure is ranging from 1 to 3.48. Those cars have more than 100 and at most 500 concatenated trips of 5 minutes (see the previous section). Note that 1598 drivers have 500 telematics trips. We show the distribution of claims number and exposures in Figure 6.1. We observe that most drivers do not have a claim. The total claims number is 933 and the total exposures is 4215. The empirical claims frequency is 22.14% per driver per year. Figure 6.1: The distributrion of claims number and exposures Remark: Our preliminary data cleaning ensures that the main driver of a car does not change over the entire observation period and we concatenate policy renewals of the same driver over this observation period. Thus, we can follow the same driver for at most 3 years and 5 months from 01/01/2014 to 31/05/2017. Remark: We follow insurance policies over multiple years, but only for the most recent periods there is telematics data available. For this reason, we typically have a longer observation period of claims history on insurance policies than of corresponding telematics data. An implicit assumption is that the driving habits and styles in the most recent periods are good representations for the entire observation period of insurance exposure. 6.1 Data preprocess We have discussed the telematics car driving data in the previous section. Here we provide a summary description of the available actuarial risk factors and partition the portfolio to facilitate the out-of-sample prediction. 6.1.1 Variable preprocess The available actuarial risk factors are regions, driver's age, driver's gender, car brand, car's age, seat count and car's price. We preprocess them as follows: regions: There are three main regions, Hebei Province, Zhejiang Province and Shanghai, which have accounted to 97.67% of total cars. Hence, we merge the regions not in the three main regions. The distribution of exposures in those four regions is shown in Figure 6.2. Note that one may create a continuous variable of population density of each region. The population density should be a factor affecting the claims frequency, since it is related to the traffic density. Figure 6.2: The distribution of exposures across the regions. driver's age: We discretize driver’s age into five groups using a marginal Poisson regression tree model. The cut-off values of age are 29, 33, 35, 45. Note that we try to obtain a finer grouping of age and merge age groups during the variable selection. Figure 6.3 shows the (marginal) Poisson tree and the distribution of exposures across the age groups. age group age interval young \\([18,29)\\) middle1 \\([29,33)\\) middle2 \\([33,35)\\) mature1 \\([35,45)\\) mature2 \\([45,100)\\) Figure 6.3: The marginal Poisson tree and the distribution of exposures across the age groups. gender: Male drivers are almost as double as female ones as shown in Figure 6.4. Figure 6.4: The distribution of exposures across the gender car brand: There are 66 different car brands. However, most car brands contain very few cars. We aggregate the car brand according to its made country. Thus we group the cars into made in Germany, Japan, China, US, Korean and Europe (except of Germany). Figure 6.5 shows the distribution of exposures across the car made countries. Figure 6.5: The distribution of exposure across car made countries. car's age: For the car’s age, we show its distribution in Figure 6.6. A preliminary analysis shows that the claims frequency is related with car’s age log-linearly. So we do not need to discretize it. Figure 6.6: The histogram of car’s age seat count: Around 95.67% cars have 5 seats as shown in Figure 6.7. So this variable is not quite useful for claims frequency prediction. Figure 6.7: The distribution of exposure across seat count. car's price: We take the logarithm of car’s price. The distribution of the car’s price in logarithm is shown in Figure 6.8. A preliminary analysis (using tree, GAM, and GLM) shows that this variable doesn’t have a close relationship with claims frequency (marginally). Figure 6.8: The histogram of car’s price average daily distance: We fit a (marginal) generalized additive model to investigate the non-linear effect of average daily distance on claims frequency. The left plot in Figure 6.9 shows that we most of the logged daily distance are between 2.5 and 4.5. Moreover, if we truncate the logged daily distance at 2.5 and 4.5, we will get a linear effect of logged daily distance on claims frequency. The right plot in Figure 6.9 confirms our propose and in the following GLM we will use the truncated logged daily distance. Figure 6.10 compares the distributions of the orginal variable and the truncated variable. Figure 6.9: The effect of logged daily distance on claims frequency. Figure 6.10: The distribution of logged daily distance. 6.1.2 Data partition We will compare models in terms of out-of-sample prediction. Stratified split (w.r.t. claims numbers) is used to partition the data into train, validation, and test data sets (\\(0.6:0.2:0.2\\)). The exposures, the claims number, and the claims frequency of each data set are listed in Table 6.1. We denote the index of each data set \\(\\mathcal{D}_\\text{train}, \\mathcal{D}_\\text{validation}, \\mathcal{D}_\\text{test}\\) by \\(\\mathcal{I}_\\text{train},\\mathcal{I}_\\text{validation},\\mathcal{I}_\\text{test}\\) and \\(\\mathcal{I}_\\text{learn}=\\mathcal{I}_\\text{train}\\cup\\mathcal{I}_\\text{validation}\\). Table 6.1: Data partitions data cars exposure claims frequency train 1104 2522 552 0.2189 validation 372 843 192 0.2279 test 371 850 189 0.2223 6.2 Generalized linear model for claim counts Generally we assume the claims number \\(N_i\\) follows a Poisson distribution with mean of \\(e_i\\lambda(\\boldsymbol{x}_i)\\) \\[N_i~ \\overset{ind}{\\sim}~ \\text{Poi} (e_i\\lambda(\\boldsymbol{x}_i)),\\] where \\(e_i\\) is the exposure and \\(\\lambda(\\boldsymbol{x}_i)\\) is the estimated claims frequency per driver per year given the risk factors \\(\\boldsymbol{x}_i\\in \\mathcal{X}\\). The function \\(\\lambda\\) is a mapping from the risk factors to the claims frequency: \\[\\lambda: \\mathcal{X} \\rightarrow \\mathbb{R}_+, ~~ \\boldsymbol{x}\\mapsto \\lambda(\\boldsymbol{x}).\\] We first establish the base line GLM for claim counts with \\(\\lambda\\) as a linear function. Then in the next two sections, we improve it by either relaxing the linear function \\(\\lambda\\) or introducing telematics covariates to expand the covariate space \\(\\mathcal{X}\\). 6.2.1 Poisson deviance loss function It is natural to use the Poisson deviance loss function to compare the prediction performance of different models. The out-of-sample Poisson deviance loss on the data \\(\\mathcal{D}_\\text{test}\\) is defined as: \\[\\mathcal{L}(\\hat{\\lambda},\\mathcal{D}_\\text{test})=\\frac{2}{|\\mathcal{D}_\\text{test}|}\\sum_{i \\in \\mathcal{I}_\\text{test}}\\left(e_i\\hat{\\lambda}(\\boldsymbol{x}_i) - N_i -N_i \\ln e_i\\hat{\\lambda}(\\boldsymbol{x}_i) + N_i\\ln N_i \\right).\\] Normally, the mapping \\(\\hat{\\lambda}\\) contains the estimated parameters using the training data \\(\\mathcal{D}_\\text{train}\\) (or the learning data \\(\\mathcal{D}_\\text{learn}\\) in GLM). 6.2.2 Generalizd linear models We begin with the golden standard model of generalized linear model. The GLM assumes the following linear mapping: \\[\\begin{equation} \\begin{aligned} \\ln \\lambda^{\\text{(GLM)}}(\\boldsymbol{x})=&amp;\\beta_0+\\alpha_\\text{region}+\\gamma_\\text{age_group}+\\zeta_\\text{female}+ \\delta_\\text{car_made}+\\\\ &amp;\\beta_1\\text{car_age}+\\beta_2\\text{logged_daily_distance}, \\end{aligned} \\end{equation}\\] where we assume Zhejiang Province, middle age 1, female, Germany made car as the reference levels, and we have discretized the age in order to obtain the log-linear effects. Note that we estimate the coefficients using the learning data \\(\\mathcal{D}_\\text{learn}\\). We then perform a step-wise variable selection according to AIC. The final model is selected as follows: \\[\\begin{equation} \\begin{aligned} \\ln \\lambda^{\\text{(GLM)}}(\\boldsymbol{x})=&amp;\\beta_0+\\alpha_\\text{hebei}+\\gamma_\\text{young}+\\gamma_\\text{middle1}+\\gamma_\\text{mature2}+\\delta_\\text{china}+\\delta_\\text{eu}+\\\\ &amp;\\beta_1\\text{car_age}+\\beta_2\\text{logged_daily_distance}, \\end{aligned} \\end{equation}\\] Hence, we have merged Shanghai Province and other regions with Zhejiang Province, middle age 2 with mature age 1, and all the car mades with Germany made except China and Europea. We have removed the gender and the car’s price. We get the test error as \\(1.0306\\). Note that test error of homogeneous model is \\(1.1003\\). 6.3 Improved GLM with boosting methods Next we explore the possibility of improving GLM using either the generalized boosted regression model or the XGBoost. The mapping \\(\\lambda\\) from actuarial risk models to claims frequency is assumed as follows: \\[\\ln \\lambda(\\boldsymbol{x})= \\ln {\\lambda}^\\text{(GLM)}(\\boldsymbol{x}) + \\ln {\\lambda}^\\text{(BST)}(\\boldsymbol{x}).\\] With the above structure, we can explore the non-linear effects and the interaction effect which are omitted in the GLM. We include the region, driver’s age (continuous variable), gender, car made, car’s age and (logged) car’s price into the boosting model \\(\\lambda^{\\text{(BST)}}\\). 6.3.1 Generalized boosted regression modeling The R code of generalized boosted regression model is shown as follows: set.seed(7) gbm1 &lt;- gbm( Claim_Count ~ BRANCHNAME + AGE + AGE_G + SEX + Car_Made + USEYEARS + Price_log + Daily_log + Daily_Tlog + offset(log(Fit_GLM)), data = rbind(train_data, valid_data), distribution = &quot;poisson&quot;, shrinkage = 0.001, n.trees = 100, interaction.depth = 1, n.minobsinnode = 100, bag.fraction = 0.5, train.fraction = nrow(train_data) / nrow(learn_data), cv.folds = 0, verbose = T ) (best.iter &lt;- gbm.perf(gbm1, method = &quot;test&quot;)) gbm1$valid.error[best.iter] The code is self explained. And we add illustrations of important arguments: offset(log(Fit_GLM)) indicates that the GBM starts boosting from the GLM prediction of claims number \\(\\ln e \\hat{\\lambda}^{\\text{(GLM)}}\\). Hence we explore the area which is not explored by the GLM. n.trees is the number of iterations (trees) we tend to boost. shrinkage is the learning step. Normally, these two variables are inversely related. It is suggested that using a small learning and a large amount of iterations will lead to a better out-of-sample performance interaction.depth is the depth of weak learner of tree. Depth of 1 implies we do not consider the interaction term. This variable needs to be tuned using the validation error. n.minobsinnode is the minimal observations in a leaf node. bag.fraction is the proportion of training data used to grow the trees. Here we choose 1 since we have a small portfolio. train.fraction indicates that the first proportion of rows are used as the train data and the rest are used as the validation data. Note that we tune the parameters shrinkage, interaction.depth by observing the changes in validation error. It turns out that interaction.depth=1 leads to the minimal validation error and shrinkage does not affect the results too much. From Figure 6.11, we see there is little improvement by the GBM. Car’s price and driver’s age has the largest two importance index indicating that we may do a better grouping of age. However, such pre-processing do not bring a much improve to the GLM. The test error is \\(1.0306\\) comparing with \\(1.0306\\) from the GLM. Figure 6.11: Calibration of gradient boosting model 6.3.2 XGBoost Similarly, we can apply the XGBoost to improve the GLM prediction. The R code is given as follows: set.seed(1) bst&lt;- xgb.train( data = dtrain, watchlist = list(train = dtrain, test = dvalid), objective = &quot;count:poisson&quot;, nrounds = 1000, eta = 0.001, max_depth = 2, min_child_weight = 100, subsample = 1, early_stopping_rounds = 5, nthread = 4, verbose = F ) bst$best_ntreelimit bst$best_msg The code is self explained. And we add illustrations of important arguments: nrounds, eta, max_depth, min_child_weight, subsample play the similar roles as n.trees, shrinkage, interaction.depth, n.mnobsinnode, bag.fraction in the GBM. early_stopping_rounds = 5 indicates that if the validation error does not improve for \\(5\\) iterations the model will stop training. Again, there is no obvious improvement found by the XGBoost (test error of \\(1.0308\\)). We conclude that the GLM can capture almost all the prediction power of the actuarial risk factors since our pre-processing of variables is appropriate and there is no obvious interaction effects. However, please note that our data only contains 1847 cars so we may not discover the potential non-linear effects and interaction effects based on such a small portfolio. 6.4 Improved GLM with telematics individual trips In this section, we apply the one dimensional convolutional neural networks (1D CNNs) to evaluate the risk associated with individual trips. One dimensional convolutional neural networks can learn the patterns from time series data. Another family of recurrent neural network including long short-term memory and gated recurrent unit can also be used to analyze time series data. Gao and Wuthrich (2019) have studied the usefulness of 1D CNNs for driver identification rather than risk evaluation of individual trips. Our proposed method include three steps: First, we calibrate a 1D CNN to classify trips of the selected archetypal drivers as either potential risky or potential safe. Second, we apply the calibrated 1D CNN to evaluate each trip of each driver and we call the sigmoid probability of the output neuron as the risk score of each trip. Third, we improve the fitted GLM for claims counts with the average risk score: \\[\\ln \\lambda(\\boldsymbol{x})= \\ln {\\lambda}^\\text{(GLM)}(\\boldsymbol{x})+ \\ln {\\lambda}^\\text{(TEL)}(\\boldsymbol{x}),\\] where the (logged) telematics modification factor \\[\\ln {\\lambda}^\\text{(TEL)}(\\boldsymbol{x})= \\beta_3+\\beta_4\\text{ave_risk_score}\\] 6.4.1 Selection of archetypal drivers Our purpose is to improve the GLM claims frequency prediction using the individual trips risk scores. Ideally, the 1D CNNs should explain some variations in the residuals from the Poisson GLM. This motivates how we select the archetypal cars and label their trips. We calculate the deviance residuals of the Poisson GLM as follows: \\[r_i=\\text{sign}(N_i-e_i\\hat{\\lambda}^{\\text{GLM}}(\\boldsymbol{x}_i)) \\sqrt{2e_i\\hat{\\lambda}^{\\text{(GLM)}}(\\boldsymbol{x}_i)-2N_i-2N_i \\ln (e_i\\hat{\\lambda}^{\\text{(GLM)}})+2N_i\\ln N_i}.\\] We draw the histogram of \\(r_i, i=1.\\ldots,n\\) in Figure 6.12. Figure 6.12: Histogram of deviance residuals We select 10 drivers with the largest residuals in the learning data set as archetypal risky drivers, and we label their 5000 trips as potential risky trips (coded as 1). We show their exposure, claims number, region, age, deviance residuals, and the GLM estimated claims frequency in Table 6.2. Table 6.2: 10 archetypal risky drivers Earned_Years Claim_Count BRANCHNAME AGE Res_D Freq_GLM 3.1836 4 other_regions 33.5 7.7461 0.2070 2.9096 4 hebei 28.0 8.8773 0.1917 2.0000 4 zhejiang 45.0 9.0390 0.2724 2.7918 3 hebei 56.0 6.7977 0.1458 2.2055 4 zhejiang 29.0 7.7946 0.2966 2.5288 3 hebei 50.0 8.3549 0.1200 3.0000 4 zhejiang 34.0 10.0270 0.1576 1.0000 2 shanghai 40.0 5.2395 0.2218 2.9507 3 hebei 29.0 5.5946 0.1748 2.0000 3 shanghai 36.0 6.4458 0.2179 For the archetypal safe drivers, we select 10 drivers with no claim and the largest exposure in the learning data set. We label their 5000 trips as potential safe trips (coded as 0). We show their information in Table 6.3. Note that we do not select drivers with the smallest residuals. Those drivers are predicted with high claims frequency but without claims. Those drivers can be potential risky drivers since even the claims frequency is high, the chance of making no claim is still high. Our experience show that if we chose drivers with the smallest residuals. We will not calibrate a useful CNN to evaluate the risk of individual trips. We denote the index set of the archetypal drivers by \\(\\mathcal{I}_{\\text{sel}}\\). Table 6.3: 10 archetypal safe drivers Earned_Years Claim_Count BRANCHNAME AGE Res_D Freq_GLM 3.4438 0 hebei 32.5 -1.4907 0.2164 3.4795 0 hebei 39.5 -0.8265 0.1188 3.4658 0 zhejiang 29.5 -2.1375 0.3084 3.4466 0 zhejiang 62.0 -1.6836 0.2442 3.4466 0 hebei 33.5 -0.7848 0.1139 3.4329 0 shanghai 44.5 -1.5932 0.2321 3.4438 0 hebei 30.5 -1.2430 0.1805 3.4795 0 hebei 35.5 -0.7124 0.1024 3.4274 0 hebei 43.5 -1.2207 0.1781 3.4603 0 hebei 23.5 -0.9866 0.1426 We add two additional telematics variables of squared acceleration rates \\(a^2\\) and squared angle changes \\(\\Delta^2\\) to the time series of individual trips. We denotes the \\(j\\)-th trip of driver \\(i\\in\\mathcal{I}_\\text{sel}\\) by \\(z_{i,j}\\in[-1,1]^{300\\times 5}\\) for \\(j=1,\\ldots,500.\\) Note that we have normalized the telematics variables \\(v, a, \\Delta, a^2, \\Delta^2\\), using the min-max normalization. We split the trips of each driver \\(i\\) into training data \\((z_{i,j})_{j=1:300}\\), the validation data \\((z_{i,j})_{j=301:400}\\), and the test data \\((z_{i,j})_{j=401:500}\\). One may use the total trips of several risky drivers and safe drivers as as test data. Our experience shows that the neural network cannot be calibrated on this kind partition of data, i.e., the validation error cannot be reduced. The reason may be that some safe drivers may have many risky trips and vice versa. 6.4.2 One dimensional convolutional neural network We label the trips of the archetypal potential risky drivers as \\(1\\) and those of the archetypal potential safe drivers as 0. We calibrate a 1D CNN \\(\\phi\\) to classify the trips of the selected archetypal drivers: \\[\\phi: [-1,1]^{300\\times5}\\rightarrow (0,1), ~~ z\\mapsto\\phi(z).\\] We call the sigmoid probability of the output neuron \\(\\phi(z)\\) as the risk score of trip \\(z\\). If the output neuron \\(\\phi(z)\\) is close to 1, then this trip gets a higher risk score. The 1D CNN is constructed as follows: Layer (type) Output Shape Param # =========================================================================== trips (InputLayer) [(None, 300, 5)] 0 ___________________________________________________________________________ cov1 (Conv1D) (None, 294, 32) 1152 ___________________________________________________________________________ ave1 (AveragePooling1D) (None, 58, 32) 0 ___________________________________________________________________________ cov2 (Conv1D) (None, 52, 16) 3600 ___________________________________________________________________________ ave2 (GlobalAveragePooling1D) (None, 16) 0 ___________________________________________________________________________ dropout (Dropout) (None, 16) 0 ___________________________________________________________________________ dense1 (Dense) (None, 8) 136 ___________________________________________________________________________ dropout_1 (Dropout) (None, 8) 0 ___________________________________________________________________________ dense2 (Dense) (None, 1) 9 =========================================================================== Total params: 4,897 Trainable params: 4,897 Non-trainable params: 0 ___________________________________________________________________________ The calibration is shown in Figures 6.13 and 6.14. We save the network weights which lead to the lowest validation loss. The validation accuracy is around \\(70\\%\\) and the test accuracy is at the same level. We plot the histogram of risk scores for test potential risky trips and for test potential safe trips in Figures @ref(fig:test_score). The test potential safe trips tend to have a lower risk scores than those riky ones. Figure 6.13: Calibration of the CNN Figure 6.14: Calibration of the CNN Figure 6.15: Calibration of the CNN Remark: In the failed trials, we find that the hyperbolic tangent activation is better than the relu activation function and the average pooling is better than the max pooling. Including the squared acceleration rates and the squared direction changes would improve the prediction accuracy of 1D CNN. The disadvantage of 1D CNN is that it is a black box and we do not know how it performs the feature engineering. 6.4.3 GLM with trips scores We then use the calibrated CNN to evaluate at most 500 individual trips of each driver. We call the sigmoid probability of the output neuron as the risk score of trips. We calculate the average risk scores \\(\\text{ave_risk_score}\\) for each car. We investigate the predictive power of risk scores of trips for claims frequency. We employ the following improved GLM: \\[\\begin{equation} \\ln \\lambda(\\boldsymbol{x})= \\ln {\\lambda}^\\text{(GLM)}(\\boldsymbol{x})+ \\ln {\\lambda}^\\text{(TEL)}(\\boldsymbol{x})=\\ln {\\lambda}^\\text{(GLM)}(\\boldsymbol{x})+ \\beta_3+\\beta_4\\text{ave_risk_score}. \\end{equation}\\] It turns out the test loss is \\(1.0287\\) comparing with \\(1.0306\\) for the GLM. The estimated telematics modification factor \\(\\lambda^{\\text{(TEL)}}\\) is \\[\\exp(\\hat{\\beta}_3+\\hat{\\beta}_4\\text{ave_risk_score})=\\exp(-0.6878+1.3712\\times\\text{ave_risk_score})\\] Our calculated risk scores are in \\((0.1511,0.7837)\\), so the modfication factor is in \\((0.6184,1.4722)\\). We plot the histogram of the telematics modification factor in Figure 6.16. Figure 6.16: Histogram of the telematics modification factors Remarks: The key point is to select the archetypal drivers. We try several options and only this works for our data. For example, one may wonder to calibrate a CNN on all drivers trips to minimize the Poisson loss. This does not work for our data no matter using the individual trips or concatenated trips. We stress that our portfolio is small which is a limitation of this analysis. 6.5 Discussions In this section, we discuss the alternatives to the proposed method. Surprisingly, if we replace the 1D CNN by a logistic regression, we would obtain an equivalent good out-of-sample prediction. 6.5.1 Trips classification by a logistic model We calculate the mean, median, minimum, maximum, quantile and standard deviation of the speed \\(v\\), acceleration \\(a\\), angle change \\(\\Delta\\), squared acceleration \\(a^2\\), and squared angle change \\(\\Delta^2\\) for each trip. Based on those manually engineered features, we implement a logistic regression to classify the trips of the selected archetypal drivers. The test accuracy is \\(66.7\\%\\) while the test accuracy of the CNN is \\(70.5\\%\\). We conclude that although the 1D CNN has a better out-of-sample prediction accuracy, the much simpler logistic regression also capture the major difference between risky trips and safe trips by using summary statistics. It seems that the chronological property in the telematics data do not play a very important role in the trips classification. 6.5.2 Competing methods We consider two competing methods: We calculate the summary statistics for each trip. Then we calculate the average of those summary statistics over all the trips for each driver. Finally, we use those averaged summary statistics as the covariates in the Poisson claims count regression. Following the same procedure as the proposed method, but replacing the 1D CNN by a logistic regression which is discussed in the previous section. This competing method uses manually feature engineering while our proposed method uses automatically feature engineering learned by the neural network. 6.5.2.1 Alternative 1: Telematics summary statistics The test Poisson loss is \\(1.0379\\) which is even worse than the GLM (\\(1.0306\\)). However, this approach performs better than the other methods in other two data partitions; see the sensitivity section. 6.5.2.2 Alternative 2: Risk scoring with logistic model The test Poisson loss is \\(1.0284\\) which is as good as the CNN method (\\(1.0287\\)). Figure 6.17 comapre the averaged risk scores from the logistic regression and the 1D CNN. The correlation between them is \\(0.9181\\). Figure 6.17: Comparison of risk scores 6.6 Sensitivity analysis and conclusions Since our portfolio is very small, we conduct a sensitivity analysis to test the robustness of our results. Similar to cross-validation, we repeat the above analysis for 5 times and evaluate test Poisson loss on 5 mutually exclusive test data sets. The results are shown in Table 6.4. Table 6.4: test Poisson loss for different data partitions test_index homo glm cnn alt_1 alt_2 1 1.1095 1.0981 1.0933 1.0773 1.0961 2 1.1003 1.0306 1.0287 1.0379 1.0284 3 1.0949 1.0641 1.0429 1.0190 1.0375 4 1.0952 1.0721 1.0656 1.0837 1.0624 5 1.0996 1.0318 1.0269 1.1082 1.0263 We conclude with the following findings: Including the telematics car driving data can improve the predictive power of the claims frequency model. The two simpler alternatives have an equivalent good predictive performance as the 1D CNN approach. The advantage of the 1D CNN in the classification of archetypal trips seems having no effect on the claims frequency modeling based on our data. "]]
